{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750c0f7e-89f4-4ddf-b1d8-2e4d6fedc8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import stuff\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"word count\") \\\n",
    "      .getOrCreate() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b39aaaf-7dfb-43f9-b7ea-31e578983184",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test if spark is running \n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9b54f6-6e0f-456f-98bc-c55a65df1203",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## tsv to dataFrame , for csv remove sep t , header option puts first line into header##\n",
    "df= spark.read.option(\"sep\",\"\\t\").csv('bands.tsv',header=True, inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae5c896-6e41-4d01-ba6e-c92992a95b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## merge df and df2 and clean\n",
    "\n",
    "##prepare data##\n",
    "df2= spark.read.option(\"sep\",\"\\t\").csv('bands2.tsv',header=True, inferSchema=True)\n",
    "\n",
    "\n",
    "    # union by name so it doesnt depend on position# \n",
    "#output= df.unionByName(df2, allowMissingColumns=True).dropDuplicates()\n",
    "\n",
    "    #clean data of nulls\n",
    "output = output.fillna(0)\n",
    "output = output.na.fill(\"\")\n",
    "output = output.drop(\"name\").withColumnRenamed(\"member\",\"Name\")\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da86fab-16cf-4b64-b2ee-43d9a4dbe267",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # join to merge data for dependencies\n",
    "output= df.join(df2,how=\"inner\")\n",
    "output = df.join(df2, on=['member','member']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18a2f78-8b2f-447e-b211-c9b275fb5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ##write dataframe to file with tab  ##\n",
    "output.write.option(\"header\",True).option(\"delimiter\",\"\\t\").csv(\"output/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c97358-9ce4-4582-8fee-2b8fc52e202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #ADD COLUMN with data\n",
    "output.withColumn(\"im Number 5\", lit(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31d251-506e-47c8-946f-07fab22feb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPARK READ FILES AND USE SOME SQL Basics##\n",
    "    #read local file\n",
    "textFile = spark.read.text(\"new.txt\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "## some interesting prompts for SQL\n",
    "#\n",
    "#\n",
    "#x = {\n",
    "#  size() counts the stuff inside\n",
    "#  split(input,seperators) splits text at seperator\n",
    "#  name(\"ColumName\") rename colum\n",
    "#  max()\n",
    "#  min()\n",
    "#  avg()\n",
    "#  lower()\n",
    "#  countDistinct()\n",
    "#}\n",
    "# .select([x])\n",
    "# .agg(x) can be used to further use aggregations \n",
    "# .distinct(x)\n",
    "# .collect() puts table into one line \n",
    "# .withColumnRenamed(from,to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31104b04-9ce6-4ca7-9c8e-63f3ae47a453",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## EXAMPLE GET SINGLE WORDS FROM FILE\n",
    "dfText = spark.read.text(\"frankenstein.txt\")\n",
    "\n",
    "df = dfText.select(split(dfText.value,\"[^a-zA-Z]\").alias(\"word\"))\n",
    "df = df.select(explode(col(\"word\")).alias(\"word\"))\n",
    "df = df.select(lower(col(\"word\")).alias(\"word\"))\n",
    "df = df.select(regexp_extract(col(\"word\"),\"[a-z]{2,}|a|i\",0).alias(\"word\"))\n",
    "df.show(50,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd14d3e4-8318-4696-a433-2b004cea8173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE GET MAX AMOUNT OF WORDS IN maxWords or in ONE LINE output #\n",
    "maxWords=textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\")))\n",
    "textFile.select(size(split(textFile.value, \"\\s+\")).name(\"numWords\")).agg(max(col(\"numWords\"))).show(100,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ecd3b57-97a0-4093-b362-36e6c8d4872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## negate search term ##\n",
    "wordFilter = [\"is\",\"not\",\"if\",\"the\"]\n",
    "newFrame = df.filter(~col(\"word\").isin(wordFilter))\n",
    "newFrame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d59b9-5c2f-47bf-80df-9096115d0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Exploding list of words into ROWS##\n",
    "from pyspark.sql.functions import explode, col\n",
    "df = df.select(explode(col(\"Zeile\")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a26ab9-8ff1-4f86-82a6-e52cdd1226c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##To lower case##\n",
    "words_lower = words.select(lower(col(\"word\")).alias(\"word_lower\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffd97da-abd1-4fc5-a938-4ba9f43d82c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## remove empty string##\n",
    "proper_words = words_clean.filter(col(\"cleanword\") != \"\")\n",
    "# or \n",
    "proper_words = words_clean.filter(~col(\"cleanword\")== \"\")\n",
    "## get word with 2 or more letters including a and i \n",
    "words_clean = words_lower.select(regexp_extract(col(\"word_lower\"), \"[a-z]{2+}|a|i\", 0).alias(\"word\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb031594-0ba8-4826-81dd-45bed6be3132",
   "metadata": {},
   "outputs": [],
   "source": [
    "##filter array##\n",
    "exclDict = [\"is\", \"not\", \"if\", \"the\", \"for\", \"of\", \"no\", \"at\", \"and\"]\n",
    "words_no_dict = words_nonull.where(~col(\"word\").isin(exclDict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a81bf2-d1ea-48e1-ab0a-e83596d79556",
   "metadata": {},
   "outputs": [],
   "source": [
    "## COMPLEX EXAMPLE ##\n",
    "esults = (\n",
    "    spark.read.text(\"frankenstein.txt\")\n",
    "    .select(F.split(F.col(\"value\"), \" \").alias(\"line\"))\n",
    "    .select(F.explode(F.col(\"line\")).alias(\"word\"))\n",
    "    .select(F.lower(F.col(\"word\")).alias(\"word\"))\n",
    "    .select(F.regexp_extract(F.col(\"word\"), \"[a-z']*\", 0).alias(\"word\"))\n",
    "    .where(F.col(\"word\") != \"\")\n",
    "    .groupby(F.col(\"word\"))\n",
    "    .count()\n",
    ")\n",
    "\n",
    "results.orderBy(\"count\", ascending=False).show(10)\n",
    "results.coalesce(1).write.csv(\"./results_single_partition.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
